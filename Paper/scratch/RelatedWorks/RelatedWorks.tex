%% 
%% 
%% 

\chapter{Related Work and Background}
\label{cha:RelatedWorks}

\section{MRFs and Energy Function}
\subsection{Markov Random Fields}
\label{sec:MRF}
\emph{Markov Random Fields} are also known as \emph{undirected
  graphical model} can be seen as a regularized joint
log-probability distribution of arbitrary non-negative functions
over a set of maximal cliques of the
graph~\cite{bishop:2006:PRML}. Let $C$ denotes a maximal clique
in one graph and $\by_C$ denotes the set of variables in that
clique. Then the joint distribution can be written as:
\begin{align}
  p(\by)=\frac{1}{Z}\prod_{C}{\Psi_C(\by_C)}
\end{align}
\noindent where $\Psi$ is called \emph{potential functions} which
can be defined as any non-negative functions and
$Z=\sum_{\by}\prod_{C}{\Psi_C(\by_C)}$ which is a normalization
constant. To infer labels which best explains input data set, we
can find the \emph{maximum a posteriori} (MAP) labels by solving
$\by^*=\argmax_{\by}p(\by)$. Because potential functions are
restricted to be non-negative, it gives us more flexible
representations by taking exponential of those terms. Thus the
joint distribution becomes:
\begin{align}
  p(\by)=\frac{1}{Z}exp(-\sum_{C}{E_C(\by_C)})
\end{align}
\noindent where $E$ is called \emph{energy functions} which can be
arbitrary functions. Therefore, \emph{maximum a posteriori}
problem is equivalent to \emph{energy minimization} problem,
which is also known as \emph{inference}:
\begin{align}
  \by^*=\argmax_{\by}p(\by)=\argmin_{\by}(-\sum_{C}{E_C(\by_C)})
\end{align}
To optimize the performance we can also consider a weighted
version of energy functions. In order to do this we can decompose
energy functions over nodes $\N$, edges $\E$ and higher order
cliques $\C$~\cite{Szummer:ECCV08} then add weights on them
accordingly. Let $\bw$ be the vector of parameters and $\phi$ be
arbitrary feature function, then the energy can be decomposed as
a set linear combinations of weights and feature vectors:

\begin{align}
  \label{eq:energyfunction_UPH}
  E(\by;\bw)=\sum_{i\in \N}{\bw_i^U\phi^U(\by_i)}+
  \sum_{(i,j)\in \E}{\bw_{ij}^P\phi^P(\by_i,\by_j)}+
  \sum_{\by_C\in \C}{\bw_C^H\phi^H(\by_C)}
\end{align}

\noindent where $U$ denotes \emph{unary} terms, $P$ denotes
\emph{pairwise} terms and $H$ denotes \emph{higher order} terms
(when $|C|>2$ namely each clique contains more than two
variables).

A weight vector $\bw$ is more preferable if it gives the
ground-truth assignments $\by_t$ less than or equal to energy
value than any other assignments $y$:

\begin{align}
E(y_t,w)\leq E(y,w)~ \text{,~}\forall y \neq y_t
\text{,~} y\in \Y
\end{align}


Thus the goal of \emph{learning} MRFs is to learn the parameter
vector $\bw^*$ which returns the lowest energy value for the
ground-truth labels $y_t$ relative to any other assignments
$y$~\cite{Szummer:ECCV08}:

\begin{align}
\bw^* = argmax_{\bw}(E(y_t,w)-E(y,w))~ \text{,~}\forall y \neq y_t
\text{,~} y\in \Y
\end{align}

So far we have introduced three main research topics of MRFs:
definition of \emph{energy function} (potential functions),
\emph{inference} problem (MAP or energy minimization) and
\emph{learning} problem.

As for energy function, our work focus on a generalization of
k-means clustering known as Gaussian Mixture Models to
incorporate information about the covariance structure of the
data as well as the centers of the latent Gaussians. The
grabcut~\cite{Rother:SIGGRAPH04} algorithm is used to train GMMs
and the final results are used to construct the unary terms. For
pairwise terms, we use the Potts model introduced by
\citename{Kohli:CVPR07} to encode pairwise consistency. The
\emph{inference} problem is solved by using a
graph-cut~\cite{Boykov:ICCV99, Boykov:PAMI04} algorithm and the
max margin framework~\cite{tsochantaridis2005large} has been
addressed to learn parameters of the energy function.

\subsection{Construction of stocks relationship graph}
\label{sec:con_stock_graph}



\subsection{Configuration of Energy Function}
\label{sec:grabcut}

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{RelatedWorks/figures/grabcut_task.png}
  \caption{\label{fig:grabcut_example} Picture on the left is
    the original picture. Picture
    on the middle is a user defined mask. The task is to extract
    foreground pixels within that rectangle. On the right is the
    ground truth foreground.}
\end{figure}

In this section we described the configuration of our energy
function. We mainly introduce the \emph{GrabCut} algorithm which
we used to generate our unary terms. The \emph{Potts model} is
used as our pairwise terms. Thus our energy function can be
written as:
\begin{align}
  \label{eq:energyfunction_UP}
  E(\by;\bw)=\theta^U\sum_{i\in \N}{\bw_i^U\phi^U(\by_i)}+
  \theta^P\sum_{(i,j)\in \E}{\bw_{ij}^P\phi^P(\by_i,\by_j)}
\end{align}
\noindent where $\theta^U$ and $\theta^P$ are parameters to be
optimized.

The \emph{GrabCut} algorithm was proposed
by~\citename{Rother:SIGGRAPH04} in order to solve background
foreground segmentation problem (see
figure~\ref{fig:grabcut_example}). They first defined MRFs over
an labeled image and then use
\emph{graph-cuts}~\cite{Boykov:ICCV01} method to do the
inference. In this section we use two of their contributions:
estimating color distribution (foreground and background) using
\emph{Gaussian Mixture Models} (GMMs) and an \emph{EM} like
two-step algorithm to train their model.

Suppose there are $N$ pixels in an image. In order to construct
MRFs, they first defined an energy
function~\eqref{eq:energyfunction_UPH} with unary and pairwise
terms:

\begin{align}
  \label{eq:grabcut_energy}
  G(\alpha, \bk, \btheta, \bz) = 
  \sum_{i\in \N}{\psi^U(\alpha_i, \bk_i, \btheta, \bz_i)}+
  \sum_{(i,j)\in \E}{\psi^P(\alpha_i,\bz_i)}
\end{align}
where $i$ is the index of pixels, $\alpha \in {0,1}$ is the label
for pixel $i$. $0$ is for the background and $1$ is for the
foreground. $\bz$ denotes the pixel vector in RGB color space.
$\bk$ and $\btheta$ are all parameters vectors and will be
explained in the next paragraph. 

\begin{algorithm}[tb]
  \begin{algorithmic}[1]
    \REPEAT
    \STATE{\emph{Assign GMM components} to stocks: \\
      $\bk_i^*=\argmin_{\bk_i}\psi^U(\alpha_i, \bk_i, \btheta,
      \bz_i)$}
    \STATE{\emph{Learn GMM parameters} from data z:\\
      $\btheta=\argmin_{\btheta}\sum_{i\in \N}{\psi^U(\alpha_i, \bk_i, \btheta, \bz_i)}$}
    \STATE{\emph{Estimate segmentation}: graph-cuts inference:\\
      $\min_{\alpha}\min_{\bk}E(\alpha, \bk, \btheta, \bz)$}
    \UNTIL{convergence}
  \end{algorithmic}
  \caption{\label{alg:grabcut} GrabCut training algorithm}
\end{algorithm}

In our configuration, we use the graph described
in~\ref{sec:con_stock_graph} in replacement of the image. Each
node in the graph represents a stock instead of pixel and the
edge between stocks represents their pairwise relationship. The
label $\alpha \in {0,1}$ equals $1$ when the stock price has a
positive movement, and vice versa. $\bz$ denotes the stock's
market price vector instead of pixel's RGB value. Other
parameters are the same with their configurations.

The \emph{Gaussian Mixture Models} (GMMs) with $K$ components
(typically $K=5$) is used for generating unary terms. Two GMMs,
one for positive movement and one for negative movement, are
jointly trained by the algorithm.
$\bk={\bk_1,\dots,\bk_i,\dots,\bk_N}$ with $\bk_i\in 1,\dots,K$
assigns each stock (node) $i$ to a unique GMMs component. The
component is either belonging to positive movement's GMMs or
negative movement's GMMs, which is depended on the label
$\alpha_i\in {0,1}$. $\btheta$ is the parameter vector which
contains parameters of standard GMMs plus \emph{mixture weighting
  coefficients}~\cite{Rother:SIGGRAPH04}.

The pairwise function $\psi^P$ in \eqref{eq:grabcut_energy} is
defined as a smoothness indicator which measures both feature
vector (stock price vector) and spatial distances (graph
distances) simultaneously. It is used to encourage coherence of
similar pixel pairs. This energy function was later used to
construct an \emph{st min-cut} graph which can be inferred
efficiently using \emph{graph-cuts}~\cite{Boykov:ICCV01}
algorithm. This gives some insights to their second contribution.

To optimize the performance, a two-step learning algorithm is
used. The algorithm first re-assign GMMs components ($\bk$) to
each pixel then update parameters $\btheta$ with new assignments.
The result of the trained GMMs are used directly into
\emph{graph-cuts} algorithm~\ref{alg:grabcut} as unary terms.
Finally the label $\alpha_i$ for each pixel $i$ is inferred
jointly using \emph{graph-cuts} algorithm. This whole procedure
is repeated until convergence (or reaches termination
conditions). We briefly summarized this procedure in
\algref{alg:grabcut}

In this thesis we use GMMs trained by GrabCut algorithm for our
unary terms $\phi^U$ in equation~\eqref{eq:energyfunction_UP}.

The pairwise function $\phi^P$ in our energy
function~\eqref{eq:energyfunction_UP} is defined as a Potts
model:

\begin{align}
  \label{eq:energyfunction_pairwise}
  \phi^P(\by_i,\by_j)=
  \begin{cases}
    0 & \text{if} x_i = x_j \\
    \psi^P_{ij}(y_i, y_j) = \frac{\lambda}{d_{ij}} \ind{y_i \neq
      y_j} \exp\left\{- \frac{\|x_i - x_j\|^2}{2 \beta}\right\} &
    \text{otherwise}
  \end{cases}
\end{align}

\noindent where $d_{ij}$ is the graph distance between stocks $i$
and $j$. $x_i$ and $x_j$ are stock market price vectors.

% %%% Local Variables:
% %%% mode: latex
% %%% TeX-master: "../thesis"
% %%% End:

